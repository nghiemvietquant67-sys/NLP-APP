## ThÃ´ng tin cÃ¡ nhÃ¢n
- **Há» vÃ  tÃªn**: NghiÃªm Viá»‡t QuÃ¢n
- **MÃ£ sinh viÃªn (MSV)**: 22001632
- **Lá»›p**: K67A4


---

## ğŸ“Œ Lab 1: Tokenization
### Ná»™i dung thá»±c hiá»‡n
- XÃ¢y dá»±ng **interface `Tokenizer`**.
- CÃ i Ä‘áº·t **SimpleTokenizer**:
  - Chuyá»ƒn chá»¯ thÆ°á»ng.
  - TÃ¡ch tá»« theo khoáº£ng tráº¯ng.
  - Xá»­ lÃ½ tÃ¡ch dáº¥u cÃ¢u Ä‘Æ¡n giáº£n (.,!?).
- CÃ i Ä‘áº·t **RegexTokenizer**:
  - Sá»­ dá»¥ng regex `\w+|[^\w\s]` Ä‘á»ƒ tÃ¡ch tá»« vÃ  dáº¥u cÃ¢u.
- Táº¡o `Lab1/main.py` Ä‘á»ƒ cháº¡y thá»­ trÃªn vÃ­ dá»¥ vÃ  dataset `UD_English-EWT`.

###  Há»c Ä‘Æ°á»£c
- Sá»± khÃ¡c biá»‡t giá»¯a **tokenizer thá»§ cÃ´ng** vÃ  **tokenizer regex**.


---

## ğŸ“Œ Lab 2: Vectorization
###  Ná»™i dung thá»±c hiá»‡n
- XÃ¢y dá»±ng **interface `Vectorizer`** vá»›i 3 phÆ°Æ¡ng thá»©c:
  - `fit(corpus)`
  - `transform(documents)`
  - `fit_transform(corpus)`
- CÃ i Ä‘áº·t **CountVectorizer**:
  - Nháº­n vÃ o má»™t `Tokenizer`.
  - Há»c **vocabulary** tá»« corpus.
  - Biáº¿n Ä‘á»•i vÄƒn báº£n thÃ nh **bag-of-words vector**.
- Táº¡o test (`test/lab2_test.py`) Ä‘á»ƒ cháº¡y vá»›i vÃ­ dá»¥.

### Há»c Ä‘Æ°á»£c
- CÃ¡ch cÃ i Ä‘áº·t thá»§ cÃ´ng mÃ´ hÃ¬nh Bag-of-Words.
- CÃ¡ch tÃ­ch há»£p tokenizer vÃ o vectorizer.

---

## ğŸ“Œ Lab 3: Trá»±c quan hÃ³a Word Embeddings vá»›i PCA vÃ  t-SNE
### Ná»™i dung thá»±c hiá»‡n
  - Sá»­ dá»¥ng mÃ´ hÃ¬nh **GloVe pre-trained** (glove-wiki-gigaword-100) vá»›i 400K tá»« vá»±ng.
  - Chá»n **5,000 tá»« phá»• biáº¿n nháº¥t** Ä‘á»ƒ trá»±c quan hÃ³a.
  - Ãp dá»¥ng **PCA (Principal Component Analysis)**:
      + Giáº£m chiá»u tá»« 100D xuá»‘ng 2D báº±ng phÆ°Æ¡ng phÃ¡p tuyáº¿n tÃ­nh.
      + Trá»±c quan hÃ³a phÃ¢n bá»‘ tá»« trong khÃ´ng gian 2D.
  - Ãp dá»¥ng **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:
      + Giáº£m chiá»u phi tuyáº¿n vá»›i perplexity=30, max_iter=1000.
      + Táº¡o cÃ¡c cluster rÃµ rÃ ng cho tá»« cÃ¹ng ngá»¯ nghÄ©a.
  - **So sÃ¡nh PCA vs t-SNE**:
      + Váº½ biá»ƒu Ä‘á»“ song song Ä‘á»ƒ quan sÃ¡t sá»± khÃ¡c biá»‡t.
      + PhÃ¢n tÃ­ch Æ°u/nhÆ°á»£c Ä‘iá»ƒm cá»§a tá»«ng phÆ°Æ¡ng phÃ¡p.
  - **Trá»±c quan hÃ³a nhÃ³m tá»« theo chá»§ Ä‘á»**
  - **TÃ¬m vÃ  trá»±c quan tá»« gáº§n nghÄ©a**

### Há»c Ä‘Æ°á»£c
  - **Ká»¹ thuáº­t giáº£m chiá»u** cho word embeddings:
      + PCA: Nhanh, á»•n Ä‘á»‹nh, báº£o toÃ n cáº¥u trÃºc toÃ n cá»¥c.
      + t-SNE: Táº¡o cluster rÃµ rÃ ng, báº£o toÃ n cáº¥u trÃºc cá»¥c bá»™.
  - **So sÃ¡nh PCA vs t-SNE**:
      + PCA: Tuyáº¿n tÃ­nh â†’ cluster khÃ´ng rÃµ rÃ ng, tá»« phÃ¢n tÃ¡n Ä‘á»u.
      + t-SNE: Phi tuyáº¿n â†’ cluster rÃµ rÃ ng, tá»« cÃ¹ng nghÄ©a nhÃ³m gáº§n nhau.
  - **Word embeddings há»c Ä‘Æ°á»£c má»‘i quan há»‡ ngá»¯ nghÄ©a**:
      + Tá»« cÃ¹ng chá»§ Ä‘á» (quá»‘c gia, Ä‘á»™ng váº­t, mÃ u sáº¯c) táº­p trung gáº§n nhau.
      + Cosine similarity hiá»‡u quáº£ Ä‘á»ƒ tÃ¬m tá»« Ä‘á»“ng nghÄ©a/gáº§n nghÄ©a.


## ğŸ“Œ Lab 4: Word Embedding
### Ná»™i dung thá»±c hiá»‡n
  1. lab4_test.py
  - XÃ¢y dá»±ng lá»›p WordEmbedder Ä‘á»ƒ lÃ m viá»‡c vá»›i word embeddings.
  - CÃ i mÃ´ hÃ¬nh pre-trained GloVe (glove-wiki-gigaword-50) báº±ng thÆ° viá»‡n gensim.
  - CÃ i Ä‘áº·t cÃ¡c hÃ m xá»­ lÃ½:
      + get_vector(word): Láº¥y vector cá»§a má»™t tá»«.
      + get_similarity(word1, word2): TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a hai tá»«.
      + get_most_similar(word): TÃ¬m cÃ¡c tá»« gáº§n nghÄ©a nháº¥t.
  - HÃ m embed_document(document): Biá»ƒu diá»…n vector cá»§a má»™t vÄƒn báº£n báº±ng cÃ¡ch láº¥y trung bÃ¬nh cá»™ng cÃ¡c vector cá»§a cÃ¡c tá»« trong vÄƒn báº£n Ä‘Ã³.
  - Táº¡o test (`test/lab4_test.py`) Ä‘á»ƒ cháº¡y vá»›i vÃ­ dá»¥.

  2. lab4_embedding_trainning_demo.py
  - XÃ¢y dá»±ng lá»›p StreamSentences Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u lá»›n tá»« file theo tá»«ng Ä‘oáº¡n (Tiáº¿t kiá»‡m RAM), tá»± Ä‘á»™ng tÃ¡ch cÃ¢u/Ä‘oáº¡n vÃ  tokenize.
  - Sá»­ dá»¥ng thÆ° viá»‡n gensim Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec tá»« trÃªn corpus UD_English-EWT.
  - Táº¡o `test/lab4_embedding_trainning_demo.py` Ä‘á»ƒ cháº¡y demo:
    + TÃ¬m cÃ¡c tá»« cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao (most_similar).
    + Giáº£i bÃ i toÃ¡n quan há»‡ tá»« (Anology) (vÃ­ dá»¥: king - man + queen = ?).

  3. lab4_spark_word2vec_demo.py
  - Sá»­ dá»¥ng Apache Spark vÃ  thÆ° viá»‡n MLlib Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u lá»›n.
  - Khá»Ÿi táº¡o SparkSession vÃ  lÃ m viá»‡c vá»›i táº­p dá»¯ liá»‡u lá»›n.
  - Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n báº±ng cÃ¡c phÃ©p biáº¿n Ä‘á»•i cá»§a Spark DataFrame:
    + Chuyá»ƒn chá»¯ thÆ°á»ng (lower).
    + Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t (regexp_replace).
  - Sá»­ dá»¥ng Tokenizer cá»§a Spark ML Ä‘á»ƒ tÃ¡ch tá»«.
  - Huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec trÃªn DataFrame Ä‘Ã£ xá»­ lÃ½ báº±ng pyspark.ml.feature.Word2Vec.
  - Sá»­ dá»¥ng phÆ°Æ¡ng thá»©c findSynonyms cá»§a mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n Ä‘á»ƒ tÃ¬m cÃ¡c tá»« tÆ°Æ¡ng Ä‘á»“ng.
  - Táº¡o `test/lab4_spark_word2vec_demo.py` Ä‘á»ƒ thá»±c thi toÃ n bá»™ pipeline xá»­ lÃ½ dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn Spark.

### Há»c Ä‘Æ°á»£c
  1. lab4_test.py
    - Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh word embedding pre-trained nhÆ° GloVe Ä‘á»ƒ biá»ƒu diá»…n vector cá»§a tá»«.
    - TÃ¬m tá»« tÆ°Æ¡ng tá»±, tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng.
    - Táº¡o vector cho vÄƒn báº£n (document embedding) tá»« cÃ¡c word embedding cÃ³ sáºµn.
  2. lab4_embedding_trainning_demo.py
    - CÃ¡ch huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh Word2Vec tá»« Ä‘áº§u báº±ng thÆ° viá»‡n gensim.
    - Sá»± khÃ¡c biá»‡t giá»¯a viá»‡c sá»­ dá»¥ng mÃ´ hÃ¬nh pre-trained (`lab4_test.py`) vÃ  tá»± huáº¥n luyá»‡n mÃ´ hÃ¬nh word embedding trÃªn dá»¯ liá»‡u riÃªng.
    - Ká»¹ thuáº­t xá»­ lÃ½ dá»¯ liá»‡u lá»›n (corpus) Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.
  3. lab4_spark_word2vec_demo.py
    - CÃ¡ch xÃ¢y dá»±ng má»™t pipeline xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vá»›i Apache Spark.
    - Sá»± khÃ¡c biá»‡t vá» tá»‘c Ä‘á»™ lÃ m viá»‡c vá»›i gensim so vá»›i Spark MLlib trÃªn dá»¯ liá»‡u quy mÃ´ lá»›n.
    - CÃ¡ch sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n cá»§a Spark ML nhÆ° Tokenizer vÃ  Word2Vec.

---

## ğŸ“Œ Lab 5: Text Classification - Sentiment Analysis
### Ná»™i dung thá»±c hiá»‡n
  1. **lab5_test.py** - TextClassifier vá»›i Scikit-learn
  - XÃ¢y dá»±ng lá»›p **TextClassifier** (`src/models/text_classifier.py`) Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n.
  - TÃ­ch há»£p vá»›i **TfidfVectorizer** Ä‘Ã£ xÃ¢y dá»±ng á»Ÿ Lab 2.
  - Sá»­ dá»¥ng **Logistic Regression** tá»« scikit-learn lÃ m mÃ´ hÃ¬nh phÃ¢n loáº¡i.
  - CÃ i Ä‘áº·t cÃ¡c phÆ°Æ¡ng thá»©c:
      + `fit(texts, labels)`: Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u.
      + `predict(texts)`: Dá»± Ä‘oÃ¡n nhÃ£n cho vÄƒn báº£n má»›i.
      + `evaluate(y_true, y_pred)`: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i cÃ¡c metrics (accuracy, precision, recall, f1-score).
  - Táº¡o test (`test/lab5_test.py`) vá»›i dá»¯ liá»‡u máº«u 6 cÃ¢u phÃ¢n loáº¡i.

  2. **lab5_spark_sentiment_analysis.py** - Baseline vá»›i PySpark
  - XÃ¢y dá»±ng pipeline phÃ¢n tÃ­ch cáº£m xÃºc baseline vá»›i **PySpark MLlib**.
  - Khá»Ÿi táº¡o **SparkSession** vÃ  Ä‘á»c dá»¯ liá»‡u tá»« `sentiments.csv`.
  - Tiá»n xá»­ lÃ½ dá»¯ liá»‡u:
      + Chuyá»ƒn Ä‘á»•i nhÃ£n tá»« {-1, 1} thÃ nh {0, 1}.
      + Loáº¡i bá» giÃ¡ trá»‹ null.
  - XÃ¢y dá»±ng Pipeline gá»“m:
      + **Tokenizer**: TÃ¡ch vÄƒn báº£n thÃ nh tá»«.
      + **StopWordsRemover**: Loáº¡i bá» stop words.
      + **HashingTF** (10,000 features): Chuyá»ƒn tá»« thÃ nh vector TF.
      + **IDF**: TÃ­nh trá»ng sá»‘ IDF.
      + **LogisticRegression**: MÃ´ hÃ¬nh phÃ¢n loáº¡i vá»›i maxIter=10, regParam=0.001.
  - ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn test set vá»›i accuracy, precision, recall, f1-score.
  - **Káº¿t quáº£**: Accuracy 72.25%.

  3. **lab5_improvement_test.py** - So sÃ¡nh vÃ  cáº£i tiáº¿n mÃ´ hÃ¬nh
  - Cáº£i tiáº¿n pipeline vá»›i **tiá»n xá»­ lÃ½ vÄƒn báº£n nÃ¢ng cao**:
      + Chuyá»ƒn chá»¯ thÆ°á»ng, loáº¡i bá» URL, HTML tags.
      + Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, chuáº©n hÃ³a khoáº£ng tráº¯ng.
  - So sÃ¡nh **3 mÃ´ hÃ¬nh**:
      + **Baseline**: HashingTF (10,000) + IDF + Logistic Regression â†’ 72.25%
      + **Improved**: Tiá»n xá»­ lÃ½ + HashingTF (2,000) + IDF + **GBTClassifier** (100 iterations) â†’ 76.29%
      + **Neural Network**: Tiá»n xá»­ lÃ½ + HashingTF (5,000) + IDF + **MLP [5000,64,32,2]** (150 iterations) â†’ **76.46%** 

### Há»c Ä‘Æ°á»£c
  1. **lab5_test.py**
    - CÃ¡ch xÃ¢y dá»±ng pipeline phÃ¢n loáº¡i vÄƒn báº£n hoÃ n chá»‰nh.
    - TÃ­ch há»£p tokenizer, vectorizer vÃ o classifier.
    - Sá»­ dá»¥ng cÃ¡c metrics Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh phÃ¢n loáº¡i.
  
  2. **lab5_spark_sentiment_analysis.py**
    - XÃ¢y dá»±ng pipeline Machine Learning vá»›i PySpark MLlib.
    - Xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n quy vá»›i Spark DataFrame.
  
  3. **lab5_improvement_test.py**
    - Táº§m quan trá»ng cá»§a **tiá»n xá»­ lÃ½ vÄƒn báº£n** vÃ  **sá»‘ lÆ°á»£ng features** (HashingTF) trong viá»‡c cáº£i thiá»‡n accuracy.
    - So sÃ¡nh hiá»‡u suáº¥t cÃ¡c mÃ´ hÃ¬nh: Logistic Regression vs GBTClassifier vs Neural Network.
    - Trade-off giá»¯a **Ä‘á»™ chÃ­nh xÃ¡c** vÃ  **thá»i gian huáº¥n luyá»‡n**.

## ğŸ“Œ Lab 5.1: Deep Learning for NLP

### Lab 5 Part 1: Introduction to PyTorch
#### Ná»™i dung thá»±c hiá»‡n
  - **Task 1: Tensor Operations**
      + Táº¡o tensor tá»« list, NumPy array, random values.
      + CÃ¡c phÃ©p toÃ¡n: cá»™ng, nhÃ¢n, matrix multiplication.
      + Indexing, slicing vÃ  reshape tensor.
      + Thuá»™c tÃ­nh: shape, dtype, device.
  - **Task 2: Autograd**
      + TÃ­nh Ä‘áº¡o hÃ m tá»± Ä‘á»™ng vá»›i `requires_grad=True`.
      + Sá»­ dá»¥ng `.backward()` Ä‘á»ƒ tÃ­nh gradient.
      + Gradient accumulation vÃ  `retain_graph=True`.
  - **Task 3: Neural Network Modules**
      + **nn.Linear**: Fully connected layer (5 dims â†’ 2 dims).
      + **nn.Embedding**: Chuyá»ƒn word indices thÃ nh dense vectors (10 tá»« â†’ 3 dims).
      + **nn.Module**: XÃ¢y dá»±ng mÃ´ hÃ¬nh tÃ¹y chá»‰nh vá»›i Embedding â†’ Linear â†’ ReLU â†’ Output.

#### Há»c Ä‘Æ°á»£c
  - Tensor operations lÃ  ná»n táº£ng cho Deep Learning.
  - Autograd giÃºp tÃ­nh gradient tá»± Ä‘á»™ng cho backpropagation.
  - XÃ¢y dá»±ng neural networks vá»›i torch.nn modules.
  - Kiáº¿n trÃºc cÆ¡ báº£n: Embedding â†’ Hidden layers â†’ Output.

---

### Lab 5 Part 2: RNNs for Text Classification
#### Ná»™i dung thá»±c hiá»‡n
  - **Dataset**: HWU64 Intent Classification (64 lá»›p intent, 8954 train / 1076 val / 1076 test).
  - **Task 1: Baseline - TF-IDF + Logistic Regression**
      + TfidfVectorizer (5000 features) + LogisticRegression.

  - **Task 2: Word2Vec + Dense Neural Network**
      + Huáº¥n luyá»‡n Word2Vec (vector_size=200, sg=1, epochs=20).
      + Chuyá»ƒn cÃ¢u thÃ nh vector trung bÃ¬nh.
      + Dense network: 200 â†’ 256 â†’ 128 â†’ 64 classes.

  - **Task 3: Pre-trained Embedding + Bi-LSTM**
      + Tokenize + Padding (max_len=50).
      + Embedding Matrix tá»« Word2Vec (trainable=False).
      + Bidirectional LSTM (128 + 64 units).

  - **Task 4: Embedding from Scratch + Bi-LSTM**
      + Embedding learnable (200 dims, trainable=True).
      + Bidirectional LSTM vá»›i same architecture.

  - **Task 5: So sÃ¡nh vÃ  Ä‘Ã¡nh giÃ¡**
      + Äá»‹nh lÆ°á»£ng: F1-score macro trÃªn test set.
      + Äá»‹nh tÃ­nh: Test trÃªn 20 cÃ¢u.

#### Há»c Ä‘Æ°á»£c
  - **RNN/LSTM** báº¯t Ä‘Æ°á»£c thá»© tá»± tá»«, tá»‘t hÆ¡n láº¥y trung bÃ¬nh cá»§a chuá»—i rá»“i phÃ¢n loáº¡i.
  - **Embedding from scratch** tá»‘t hÆ¡n pre-trained khi cÃ³ Ä‘á»§ data vÃ  task chuyÃªn biá»‡t.
  - **Bidirectional LSTM** máº¡nh máº½ vá»›i text classification (context 2 chiá»u).
  - CÃ¡c ká»¹ thuáº­t: Dropout, BatchNorm, EarlyStopping, ReduceLROnPlateau.

---

### Lab 5 Part 3: RNNs for POS Tagging
#### Ná»™i dung thá»±c hiá»‡n
  - **Dataset**: UD English-EWT (Universal Dependencies) - 17 UPOS tags.
  - **Task 1: Chuáº©n bá»‹ dá»¯ liá»‡u**
      + Load CoNLL-U format (train: 12,543 cÃ¢u / dev: 2,001 / test: 2,077).
      + XÃ¢y dá»±ng vocabulary: 19,675 tá»«, 17 POS tags.
  - **Task 2: PyTorch Dataset vÃ  DataLoader**
      + Custom POSDataset class.
      + Padding vá»›i `pad_sequence` vÃ  `collate_fn`.
      + DataLoader vá»›i batch_size=32.
  - **Task 3: XÃ¢y dá»±ng mÃ´ hÃ¬nh SimpleRNN**
      + Embedding (19675 â†’ 128 dims).
      + RNN (128 â†’ 256 hidden).
      + Linear (256 â†’ 17 tags).
  - **Task 4: Huáº¥n luyá»‡n**
      + Optimizer: Adam (lr=0.001).
      + Loss: CrossEntropyLoss vá»›i ignore_index=PAD_TAG_ID.
      + Training 5 epochs vá»›i masking.
  - **Task 5: ÄÃ¡nh giÃ¡**
      + **Dev Accuracy**: 88.46%.
      + **Test Accuracy**: 88.15%.
      + Dá»± Ä‘oÃ¡n cÃ¢u má»›i: "The quick brown fox jumps over the lazy dog".

#### Há»c Ä‘Æ°á»£c
  - **Sequence labeling** khÃ¡c text classification (má»—i token cÃ³ 1 label).
  - **Padding vÃ  Masking** quan trá»ng Ä‘á»ƒ xá»­ lÃ½ cÃ¢u khÃ¡c Ä‘á»™ dÃ i.
  - **RNN** phÃ¹ há»£p cho POS tagging (báº¯t Ä‘Æ°á»£c temporal dependencies).

---

### Lab 5 Part 4: RNNs for Named Entity Recognition (NER)
#### Ná»™i dung thá»±c hiá»‡n
  - **Dataset**: CoNLL-2003 NER (14,041 train / 3,250 validation / 3,453 test).
  - **NhÃ£n**: 9 NER tags (O, B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC, B-MISC, I-MISC).
  - **Task 1: Chuáº©n bá»‹ dá»¯ liá»‡u**
      + Load dataset tá»« Hugging Face datasets.
      + Chuyá»ƒn Ä‘á»•i nhÃ£n sá»‘ sang nhÃ£n string.
      + XÃ¢y dá»±ng vocabulary: 23,624 tá»«, 10 tags (9 NER + PAD).
  - **Task 2: PyTorch Dataset vÃ  DataLoader**
      + Custom NERDataset class.
      + Padding vá»›i attention mask.
      + DataLoader batch_size=32.
  - **Task 3: XÃ¢y dá»±ng Bidirectional LSTM**
      + Embedding (23624 â†’ 128 dims).
      + Bidirectional LSTM (128 â†’ 256Ã—2 hidden, dropout=0.5).
      + Linear (512 â†’ 10 tags).
  - **Task 4: Huáº¥n luyá»‡n 5 epochs**
      + Loss giáº£m: 0.4993 â†’ 0.0208.
      + Train accuracy: 92.45% â†’ 99.90%.
      + Valid accuracy: 90.90% â†’ 94.69%.
  - **Task 5: ÄÃ¡nh giÃ¡**
      + **Validation**: Accuracy 94.69%, Precision 76.75%, Recall 70.23%, F1 73.35%.
      + **Test**: Accuracy 92.67%, Precision 68.51%, Recall 61.60%, F1 64.87%.
      + Dá»± Ä‘oÃ¡n cÃ¡c cÃ¢u má»›i.

#### Há»c Ä‘Æ°á»£c
  - **Bidirectional LSTM** hiá»‡u quáº£ cho NER (context 2 chiá»u).
  - Sá»­ dá»¥ng **seqeval** cho entity-level metrics.
  - NER phá»©c táº¡p hÆ¡n POS tagging vÃ¬ cáº§n nháº­n dáº¡ng cáº£ ranh giá»›i thá»±c thá»ƒ.

---

## ğŸ“Œ Lab 6: Transformers vÃ  Hugging Face
### Ná»™i dung thá»±c hiá»‡n
  - **BÃ i 1: Masked Language Modeling (MLM)**
      + Sá»­ dá»¥ng pipeline "fill-mask" vá»›i BERT.
      + Test cÃ¢u: "Hanoi is the `<mask>` of Vietnam".
      + **Káº¿t quáº£**: Dá»± Ä‘oÃ¡n "capital" vá»›i Ä‘á»™ tin cáº­y 40.33%.
  
  - **BÃ i 2: Text Generation**
      + Sá»­ dá»¥ng pipeline "text-generation" vá»›i GPT-2.
      + Prompt: "The best thing about learning NLP is".
      + **Káº¿t quáº£**: VÄƒn báº£n máº¡ch láº¡c nhÆ°ng chÆ°a tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m.

  - **BÃ i 3: Sentence Embedding**
      + Load mÃ´ hÃ¬nh BERT (bert-base-uncased, 768 hidden dims).
      + Tokenize cÃ¢u: "This is a sample sentence.".
      + Láº¥y hidden states tá»« last layer.
      + **Mean Pooling** vá»›i attention_mask.
      + **Káº¿t quáº£**: Vector 768 chiá»u.

### Há»c Ä‘Æ°á»£c
  - **Ba kiáº¿n trÃºc Transformer**:
      + Encoder-only (BERT): Hiá»ƒu ngá»¯ cáº£nh â†’ Classification, NER, QA.
      + Decoder-only (GPT): Sinh vÄƒn báº£n â†’ Text generation.
      + Encoder-Decoder (T5): Seq2seq â†’ Translation, Summarization.
  - **Hugging Face Transformers**:
      + Pipeline API Ä‘Æ¡n giáº£n hÃ³a viá»‡c sá»­ dá»¥ng mÃ´ hÃ¬nh.
  - **Masked Language Modeling**: BERT dá»± Ä‘oÃ¡n tá»« bá»‹ mask báº±ng bidirectional context.
  - **Text Generation**: GPT sinh vÄƒn báº£n.
  - **Sentence Embedding**: Mean pooling vá»›i attention mask táº¡o vector biá»ƒu diá»…n cÃ¢u.

---

## ğŸ“Œ Lab 7: Dependency Parsing
### Ná»™i dung thá»±c hiá»‡n
  - **Pháº§n 1: CÃ i Ä‘áº·t**
      + CÃ i Ä‘áº·t thÆ° viá»‡n **spaCy**.
      + Táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯ tiáº¿ng Anh `en_core_web_sm`.
  
  - **Pháº§n 2: PhÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a**
      + Load mÃ´ hÃ¬nh spaCy vÃ  phÃ¢n tÃ­ch cÃ¢u.
      + Sá»­ dá»¥ng **displaCy** Ä‘á»ƒ trá»±c quan hÃ³a cÃ¢y phá»¥ thuá»™c.
      + Khá»Ÿi cháº¡y server táº¡i http://127.0.0.1:5000 Ä‘á»ƒ xem cÃ¢y phá»¥ thuá»™c.
  
  - **Pháº§n 3: Truy cáº­p cÃ¡c thÃ nh pháº§n trong cÃ¢y**
      + Truy cáº­p cÃ¡c thuá»™c tÃ­nh token: `text`, `pos_`, `dep_`, `head`, `children`.
      + PhÃ¢n tÃ­ch cÃ¢u "Apple is looking at buying U.K. startup for $1 billion".
      + Hiá»ƒu quan há»‡ giá»¯a token, head vÃ  children trong cÃ¢y phá»¥ thuá»™c.
  
  - **Pháº§n 4: Duyá»‡t cÃ¢y Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin**
      + **BÃ i toÃ¡n 4.1**: TÃ¬m chá»§ ngá»¯ (nsubj) vÃ  tÃ¢n ngá»¯ (dobj) cá»§a Ä‘á»™ng tá»«.
        - Test: "The cat chased the mouse and the dog watched them."
        - Káº¿t quáº£: (cat, chased, mouse) vÃ  (dog, watched, them).
      + **BÃ i toÃ¡n 4.2**: TÃ¬m tÃ­nh tá»« bá»• nghÄ©a (amod) cho danh tá»«.
        - Test: "The big, fluffy white cat is sleeping on the warm mat."
        - Káº¿t quáº£: cat â†’ [big, fluffy, white], mat â†’ [warm].
  
  - **Pháº§n 5: BÃ i táº­p tá»± luyá»‡n**
      + **BÃ i 1**: TÃ¬m Ä‘á»™ng tá»« chÃ­nh (ROOT) cá»§a cÃ¢u.
        - Viáº¿t hÃ m `find_main_verb(doc)` tráº£ vá» token cÃ³ `dep_ == "ROOT"`.
        - Test 4 cÃ¢u vÃ  hiá»ƒn thá»‹ Ä‘á»™ng tá»« chÃ­nh, POS tag, lemma.
      
      + **BÃ i 2**: TrÃ­ch xuáº¥t cÃ¡c cá»¥m danh tá»« (Noun Chunks).
        - Viáº¿t hÃ m `extract_noun_chunks(doc)` tá»± Ä‘á»™ng trÃ­ch xuáº¥t cá»¥m danh tá»«.
        - Thu tháº­p danh tá»« vÃ  cÃ¡c tá»« bá»• nghÄ©a: det, amod, compound, nummod, poss.
        - So sÃ¡nh káº¿t quáº£ vá»›i `.noun_chunks` cÃ³ sáºµn cá»§a spaCy.
      
      + **BÃ i 3**: TÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t trong cÃ¢y.
        - Viáº¿t hÃ m `get_path_to_root(token)` tÃ¬m Ä‘Æ°á»ng Ä‘i tá»« token lÃªn ROOT.
        - Viáº¿t hÃ m `get_distance_to_root(token)` tÃ­nh sá»‘ bÆ°á»›c Ä‘áº¿n ROOT.

### Há»c Ä‘Æ°á»£c
  - **Dependency Parsing**: Hiá»ƒu cáº¥u trÃºc cÃ¢y phá»¥ thuá»™c vÃ  cÃ¡c quan há»‡ giá»¯a tá»«.
  - **spaCy**: Sá»­ dá»¥ng thÆ° viá»‡n máº¡nh máº½ cho phÃ¢n tÃ­ch cÃº phÃ¡p phá»¥ thuá»™c.
  - **displaCy**: Trá»±c quan hÃ³a cÃ¢y phá»¥ thuá»™c giÃºp hiá»ƒu rÃµ cáº¥u trÃºc cÃ¢u.
  - **TrÃ­ch xuáº¥t thÃ´ng tin**: TÃ¬m Subject-Verb-Object, tÃ­nh tá»« bá»• nghÄ©a, noun chunks.
  - **Thuáº­t toÃ¡n trÃªn cÃ¢y**: Duyá»‡t cÃ¢y, tÃ¬m Ä‘Æ°á»ng Ä‘i, tÃ­nh khoáº£ng cÃ¡ch, LCA.
  - **á»¨ng dá»¥ng thá»±c táº¿**: Information Extraction, Question Answering, Knowledge Graph Construction.

---

## ğŸ“Œ Lab X: Text-to-Speech (NghiÃªn cá»©u)
### Ná»™i dung nghiÃªn cá»©u
  - **Lá»‹ch sá»­ phÃ¡t triá»ƒn TTS**:
      + Giai Ä‘oáº¡n truyá»n thá»‘ng: Concatenative, Formant, Articulatory Synthesis.
      + Ká»· nguyÃªn Deep Learning: WaveNet (2016), Tacotron (2017).
      + Tháº¿ há»‡ hiá»‡n Ä‘áº¡i: Tacotron 2, FastSpeech, VITS, Neural Codec Models.
  
  - **Kiáº¿n trÃºc TTS hiá»‡n Ä‘áº¡i**:
      + Pipeline: Text Analysis â†’ Acoustic Model â†’ Vocoder â†’ Audio.
      + **Tacotron 2**: Character Embeddings â†’ Encoder â†’ Attention â†’ Decoder â†’ Post-net.
      + **FastSpeech 2**: Feed-Forward Transformer + Variance Adaptors (duration, pitch, energy).
      + **HiFi-GAN**: GAN-based vocoder vá»›i Multi-receptive field fusion.
  
  - **Ká»¹ thuáº­t nÃ¢ng cao**:
      + Multi-Speaker TTS: Speaker embeddings, Zero-shot voice cloning.
      + TTS biá»ƒu cáº£m: Style Tokens, Reference Encoder, emotion control.
      + Cross-lingual TTS: IPA phonemes, language embeddings, transfer learning.
  
  - **ThÃ¡ch thá»©c & TÆ°Æ¡ng lai**:
      + Ngá»¯ Ä‘iá»‡u tá»± nhiÃªn, zero-shot cloning, mÃ´ hÃ¬nh thá»‘ng nháº¥t.
      + Hiá»‡u nÄƒng on-device, phÃ¡t hiá»‡n Deepfake, watermarking.

---

