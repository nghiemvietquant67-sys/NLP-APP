# BÃ¡o cÃ¡o Lab 2 â€” Pipeline NLP vá»›i Spark

**Harito ID:** 2025-09-25 / 2025-10-02  
**NgÃ y:** 2025-12-17

---

## âœ… TÃ³m táº¯t
TÃ´i Ä‘Ã£ triá»ƒn khai má»™t pipeline dá»±a trÃªn PySpark cho Lab 2: Ä‘á»c file JSON C4, tokenize vÄƒn báº£n (máº·c Ä‘á»‹nh dÃ¹ng `RegexTokenizer`), loáº¡i bá» stop words, tÃ­nh `HashingTF` â†’ `IDF`, chuáº©n hÃ³a vector TF-IDF vÃ  lÆ°u máº«u káº¿t quáº£. Repository hiá»‡n chá»©a:

- `spark_labs/lab2_pipeline.py` â€” triá»ƒn khai pipeline PySpark âœ…
- `test/test_lab2.py` â€” test smoke nháº¹ âœ…
- `report/lab2_report.md` â€” bÃ¡o cÃ¡o nÃ y âœ…

---

## CÃ¡c bÆ°á»›c triá»ƒn khai (code) ğŸ”§
1. PhÃ¡t hiá»‡n cá»™t chá»©a vÄƒn báº£n trong JSON Ä‘áº§u vÃ o vÃ  Ä‘á»•i tÃªn thÃ nh `text` Ä‘á»ƒ Ä‘á»“ng nháº¥t.
2. XÃ¢y dá»±ng `Pipeline` vá»›i cÃ¡c stage cÃ³ thá»ƒ cáº¥u hÃ¬nh:
   - `RegexTokenizer` (hoáº·c `Tokenizer`) â†’ `tokens`
   - `StopWordsRemover` â†’ `filtered`
   - `HashingTF(numFeatures=...)` â†’ `rawFeatures`
   - `IDF` â†’ `tfidf`
   - `Normalizer` â†’ `norm`
3. `fit` vÃ  `transform` dataset (cÃ³ thá»ƒ giá»›i háº¡n báº±ng tham sá»‘ `--limitDocuments` Ä‘á»ƒ cháº¡y nhanh khi phÃ¡t triá»ƒn).
4. LÆ°u má»™t máº«u nhá» káº¿t quáº£ vÃ o `results/lab2_pipeline_output.txt`.
5. TÃ­nh top-5 document tÆ°Æ¡ng Ä‘á»“ng (cosine similarity) cho má»™t document Ä‘Æ°á»£c chá»n báº±ng cÃ¡ch collect cÃ¡c vector Ä‘Ã£ chuáº©n hÃ³a vá» driver.
6. Ghi thá»i gian tá»«ng stage vÃ  lá»—i vÃ o `log/lab2_run.log`.

---

## CÃ¡ch cháº¡y (PowerShell) â–¶ï¸
1. CÃ i Java (JDK 17+) vÃ  Python (3.8+).
2. CÃ i PySpark: `pip install pyspark`.
3. Tá»« thÆ° má»¥c gá»‘c repo cháº¡y:

```powershell
python spark_labs/lab2_pipeline.py --input data/c4-train.00000-of-01024-30K.json --limitDocuments 1000 --numFeatures 20000 --use_regex True --output results/lab2_pipeline_output.txt
```

- Äá»ƒ cháº¡y nhanh (smoke run), dÃ¹ng `--limitDocuments 100` vÃ  `--numFeatures 1000`.
- File log: `log/lab2_run.log` chá»©a thá»i gian báº¯t Ä‘áº§u/káº¿t thÃºc vÃ  thá»i lÆ°á»£ng tá»«ng stage.

---

## Káº¿t quáº£ mong Ä‘á»£i & diá»…n giáº£i ğŸ§¾
- File output chá»©a cÃ¡c dÃ²ng theo dáº¡ng: `_id \t text_preview \t Vector([...])`.
- Vector TF-IDF biá»ƒu thá»‹ táº§m quan trá»ng cá»§a tá»« trong document; sau khi chuáº©n hÃ³a (Normalizer) cÃ¡c vector cÃ³ thá»ƒ so sÃ¡nh báº±ng cosine similarity.
- Káº¿t quáº£ similarity liá»‡t kÃª top-5 document gáº§n nháº¥t cho document Ä‘Æ°á»£c chá»n (há»¯u Ã­ch cho retrieval hoáº·c nearest-neighbor tasks).

---

## KhÃ³ khÄƒn & giáº£i phÃ¡p âš ï¸
- Háº¡n cháº¿ mÃ´i trÆ°á»ng: PySpark yÃªu cáº§u Java; mÃ´i trÆ°á»ng CI ban Ä‘áº§u thiáº¿u Python hoáº·c PySpark. Giáº£i phÃ¡p: cung cáº¥p test smoke khÃ´ng cáº§n dataset lá»›n vÃ  mÃ´ táº£ rÃµ cÃ¡c bÆ°á»›c cÃ i Ä‘áº·t.
- Xá»­ lÃ½ dataset lá»›n: tÃ­nh toÃ¡n similarity trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u ráº¥t tá»‘n kÃ©m. Giáº£i phÃ¡p: giá»›i háº¡n sá»‘ document báº±ng `--limitDocuments` vÃ  tÃ­nh similarity báº±ng cÃ¡ch collect xuá»‘ng driver cho má»¥c Ä‘Ã­ch lab.

---

## TÃ i liá»‡u tham kháº£o & ghi chÃº ğŸ“š
- TÃ i liá»‡u Spark MLlib: https://spark.apache.org/docs/latest/ml-classification-regression.html  
- CÃ¡c lá»›p PySpark feature: `RegexTokenizer`, `StopWordsRemover`, `HashingTF`, `IDF`, `Normalizer`  

---

Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ: (A) thÃªm workflow CI Ä‘á»ƒ cháº¡y test smoke khi push, hoáº·c (B) má»Ÿ rá»™ng báº±ng pháº§n LogisticRegression Ä‘á»ƒ phÃ¢n loáº¡i; báº¡n muá»‘n tÃ´i lÃ m gÃ¬ tiáº¿p theo? ğŸ”§
