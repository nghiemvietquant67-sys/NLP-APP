# BÃ¡o cÃ¡o Lab 3 â€” Word Embeddings (Gensim + PySpark)

**Harito ID:** 2025-09-25  
**NgÃ y:** 2025-12-17

---

## âœ… TÃ³m táº¯t
BÃ i lab nÃ y thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m embedding tá»« vá»›i **gensim** (dÃ¹ng model GloVe Ä‘Ã£ huáº¥n luyá»‡n sáºµn vÃ  huáº¥n luyá»‡n Word2Vec trÃªn bá»™ dá»¯ liá»‡u nhá») cÃ¹ng má»™t demo PySpark Ä‘á»ƒ má»Ÿ rá»™ng huáº¥n luyá»‡n Word2Vec. Repository bao gá»“m:

- `src/representations/word_embedder.py` â€” lá»›p `WordEmbedder` táº£i `glove-wiki-gigaword-50`, cung cáº¥p `get_vector`, `get_similarity`, `get_most_similar`, vÃ  `embed_document`. âœ…
- `test/lab4_test.py` â€” test/demo smoke in ra vÃ­ dá»¥ (vector cá»§a `king`, similarity, tá»« tÆ°Æ¡ng tá»±, embedding document). âœ…
- `test/lab4_embedding_training_demo.py` â€” huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh Word2Vec nhá» trÃªn `UD_English-EWT` vÃ  lÆ°u láº¡i. âœ…
- `test/lab4_spark_word2vec_demo.py` â€” vÃ­ dá»¥ PySpark tokenization trÃªn C4 vÃ  huáº¥n luyá»‡n Word2Vec (nÃ¢ng cao). âœ…
- `notebook/Lab_3_Word_Embeddings.ipynb` â€” notebook tÆ°Æ¡ng tÃ¡c vá»›i vÃ­ dá»¥ vÃ  hÆ°á»›ng dáº«n. âœ…

**Tiáº¿n Ä‘á»™ so vá»›i checklist:** 10/12 má»¥c Ä‘Ã£ hoÃ n thÃ nh (pháº§n visualization cÃ²n Ä‘á»ƒ thÃªm vÃ o notebook).

---

## Triá»ƒn khai (nhá»¯ng gÃ¬ Ä‘Ã£ lÃ m) ğŸ”§
1. Task 1 â€” Embeddings Ä‘Ã£ huáº¥n luyá»‡n sáºµn (Gensim)
   - Triá»ƒn khai `WordEmbedder` sá»­ dá»¥ng `gensim.downloader.load(model_name)`; máº·c Ä‘á»‹nh `glove-wiki-gigaword-50` (50 chiá»u).
   - CÃ¡c phÆ°Æ¡ng thá»©c: `get_vector(word)`, `get_similarity(word1, word2)`, `get_most_similar(word, top_n)`.

2. Task 2 â€” Embedding document
   - Triá»ƒn khai `embed_document(document, tokenizer)` trung bÃ¬nh cÃ¡c vector tá»« biáº¿t (bá» qua token OOV) vÃ  tráº£ vá» vector zero náº¿u khÃ´ng cÃ³ token há»£p lá»‡.

3. Task 3 â€” Huáº¥n luyá»‡n Word2Vec (gensim)
   - `test/lab4_embedding_training_demo.py` stream vÄƒn báº£n tá»« `data/UD_English-EWT/en_ewt-ud-train.txt`, huáº¥n luyá»‡n Word2Vec nhá» (`vector_size=50`) vÃ  lÆ°u sang `results/word2vec_ewt.model`.

4. Task 4 â€” Huáº¥n luyá»‡n Word2Vec vá»›i Spark (nÃ¢ng cao)
   - `test/lab4_spark_word2vec_demo.py` minh hoáº¡ Ä‘á»c `data/c4-train...json`, lÃ m sáº¡ch & tokenization Ä‘Æ¡n giáº£n (lowercase, loáº¡i dáº¥u cÃ¢u) vÃ  huáº¥n luyá»‡n `pyspark.ml.feature.Word2Vec` (vectorSize=100). ÄÃ¢y lÃ  demo tá»‘i thiá»ƒu cÃ³ thá»ƒ má»Ÿ rá»™ng cho dá»¯ liá»‡u lá»›n hÆ¡n.

5. Task 5 â€” Visualization (PCA / t-SNE)
   - ChÆ°a thÃªm vÃ o notebook (bÆ°á»›c káº¿ tiáº¿p). TÃ´i Ä‘Ã£ chuáº©n bá»‹ snippet code chi tiáº¿t trong pháº§n â€œCÃ¡ch tÃ¡i táº¡o & Visualizationâ€ bÃªn dÆ°á»›i Ä‘á»ƒ báº¡n cháº¡y cá»¥c bá»™.

---

## CÃ¡ch cháº¡y (tÃ¡i táº¡o) â–¶ï¸
YÃªu cáº§u:
- Python 3.8+ vÃ  pip
- CÃ i dependencies: `pip install -r requirements.txt` (sáº½ cÃ i `gensim`; vá»›i job Spark cáº§n thÃªm `pyspark` náº¿u cháº¡y demo Spark)
- Láº§n cháº¡y Ä‘áº§u tiÃªn model pre-trained sáº½ táº£i `glove-wiki-gigaword-50` (~65MB).

CÃ¡c lá»‡nh / demo:
- Cháº¡y vÃ­ dá»¥ smoke (táº£i model láº§n Ä‘áº§u):
  ```powershell
  python test/lab4_test.py
  ```
  Káº¿t quáº£ mong Ä‘á»£i: in ra kÃ­ch thÆ°á»›c vector cá»§a `king`, giÃ¡ trá»‹ similarity cho `king<>queen` vÃ  `king<>man`, top-10 tá»« tÆ°Æ¡ng tá»± `computer`, vÃ  vector embedding cá»§a má»™t document.

- Huáº¥n luyá»‡n Word2Vec nhá» vá»›i gensim trÃªn UD_English-EWT (demo):
  ```powershell
  python test/lab4_embedding_training_demo.py
  ```
  Káº¿t quáº£: file `results/word2vec_ewt.model` vÃ  má»™t vÃ­ dá»¥ cÃ¡c tá»« tÆ°Æ¡ng tá»± Ä‘Æ°á»£c in ra.

- Cháº¡y demo PySpark Word2Vec (cáº§n Java + pyspark):
  ```powershell
  python test/lab4_spark_word2vec_demo.py
  ```

Notebook (khÃ¡m phÃ¡ tÆ°Æ¡ng tÃ¡c):
- Má»Ÿ `notebook/Lab_3_Word_Embeddings.ipynb` trong Jupyter Lab hoáº·c VS Code vÃ  cháº¡y cÃ¡c cell theo thá»© tá»±.

---

## Káº¿t quáº£ & PhÃ¢n tÃ­ch (mong Ä‘á»£i) ğŸ“Š
- VÃ­ dá»¥ giÃ¡ trá»‹ similarity (xáº¥p xá»‰):
  - `sim(king, queen)` thÆ°á»ng cao (gáº§n 0.7â€“0.8 vá»›i GloVe-50) â€” pháº£n Ã¡nh quan há»‡ giá»›i/tÆ°á»›c vá»‹.
  - `sim(king, man)` thÆ°á»ng tháº¥p hÆ¡n má»™t chÃºt so vá»›i king<>queen nhÆ°ng váº«n cao.
- `get_most_similar('computer')` tráº£ vá» cÃ¡c tá»« thuá»™c lÄ©nh vá»±c mÃ¡y tÃ­nh (vÃ­ dá»¥: 'computers', 'software', 'hardware', 'pc').
- Embedding document (mean cá»§a cÃ¡c vector tá»«) táº¡o ra vector dÃ y 50 chiá»u; phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ similarity vÃ  clustering.

Ghi chÃº vá» model tá»± huáº¥n luyá»‡n vs model pre-trained:
- Model pre-trained (GloVe) náº¯m báº¯t quan há»‡ ngá»¯ nghÄ©a rá»™ng rÃ£i tá»« táº­p dá»¯ liá»‡u lá»›n â€” phÃ¹ há»£p cho ngá»¯ nghÄ©a tá»•ng quÃ¡t.
- Model tá»± huáº¥n luyá»‡n (Word2Vec trÃªn EWT) sáº½ pháº£n Ã¡nh quan há»‡ Ä‘áº·c thÃ¹ domain/genre cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n (há»¯u Ã­ch náº¿u domain má»¥c tiÃªu tÆ°Æ¡ng Ä‘á»“ng) nhÆ°ng kÃ©m bá»n vá»¯ng náº¿u dá»¯ liá»‡u nhá».

---

## Visualization (cÃ¡ch táº¡o Ä‘á»“ thá»‹ cá»¥c bá»™) âœ¨
DÃ¹ng PCA hoáº·c t-SNE Ä‘á»ƒ giáº£m vector tá»« vá» 2 chiá»u vÃ  váº½ scatter plot.
VÃ­ dá»¥ (cháº¡y trong notebook):

```python
# láº¥y vectors cho cÃ¡c tá»« chá»n lá»c
words = ['king','queen','man','woman','computer','software','apple','orange','bank','river']
vectors = [we.get_vector(w) for w in words]

# PCA (nhanh)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
proj = pca.fit_transform(vectors)

import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.scatter(proj[:,0], proj[:,1])
for i,w in enumerate(words):
    plt.text(proj[i,0]+0.01, proj[i,1]+0.01, w)
plt.title('PCA of selected word vectors')
plt.show()

# TÃ¹y chá»n: t-SNE cho bá»‘ cá»¥c phi tuyáº¿n
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
tsne_proj = tsne.fit_transform(vectors)
# váº½ tÆ°Æ¡ng tá»± nhÆ° PCA
```

Diá»…n giáº£i:
- Nhá»¯ng tá»« cÃ³ liÃªn quan (vÃ­ dá»¥: `king` & `queen`, `computer` & `software`) thÆ°á»ng nhÃ³m gáº§n nhau; cáº·p tá»« liÃªn quan tá»›i giá»›i (gender) cÃ³ thá»ƒ náº±m theo cÃ¹ng má»™t hÆ°á»›ng.

---

## KhÃ³ khÄƒn & Giáº£i phÃ¡p âš ï¸
- KÃ­ch thÆ°á»›c/tá»‘c Ä‘á»™ táº£i model: model GloVe (~65MB) cáº§n táº£i láº§n Ä‘áº§u â€” ghi chÃº rÃµ trong notebook Ä‘á»ƒ cáº£nh bÃ¡o ngÆ°á»i dÃ¹ng.
- OOV tokens: `get_vector` há»— trá»£ lookup khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng vÃ  tráº£ `None` náº¿u OOV; `embed_document` bá» qua OOV vÃ  tráº£ vector zero náº¿u khÃ´ng cÃ³ token há»£p lá»‡.
- Qui mÃ´ huáº¥n luyá»‡n: huáº¥n luyá»‡n Word2Vec trÃªn toÃ n bá»™ C4 Ä‘Ã²i há»i tÃ i nguyÃªn phÃ¢n tÃ¡n; demo PySpark lÃ  bÆ°á»›c báº¯t Ä‘áº§u tá»‘i thiá»ƒu â€” Ä‘á»ƒ huáº¥n luyá»‡n quy mÃ´ lá»›n cáº§n cluster vÃ  Ä‘iá»u chá»‰nh `minCount`, `vectorSize`, sá»‘ worker.

---

## Tests & Kiá»ƒm nghiá»‡m âœ…
- Cháº¡y `python test/lab4_test.py` Ä‘á»ƒ kiá»ƒm tra nhanh (in vÃ­ dá»¥).
- Cháº¡y `python test/lab4_embedding_training_demo.py` Ä‘á»ƒ huáº¥n luyá»‡n vÃ  kiá»ƒm nghiá»‡m mÃ´ hÃ¬nh Word2Vec nhá» trÃªn UD_English-EWT.
- Cháº¡y `python test/lab4_spark_word2vec_demo.py` trÃªn mÃ¡y cÃ³ Java + pyspark Ä‘á»ƒ kiá»ƒm tra luá»“ng Spark.

---

## TÃ i liá»‡u tham kháº£o & Ä‘á»c thÃªm ğŸ“š
- GloVe: Pennington, Socher, Manning (2014): https://nlp.stanford.edu/projects/glove/  
- Word2Vec: Mikolov et al. (2013): https://arxiv.org/abs/1301.3781  
- Gensim docs: https://radimrehurek.com/gensim/  
- PySpark MLlib Word2Vec docs: https://spark.apache.org/docs/latest/ml-features.html#word2vec

---

## BÆ°á»›c tiáº¿p theo (tuá»³ chá»n)
- ThÃªm cell visualization vÃ o `notebook/Lab_3_Word_Embeddings.ipynb` (tÃ´i cÃ³ thá»ƒ thÃªm code PCA & t-SNE cÃ¹ng áº£nh máº«u vÃ  commit).
- ThÃªm pháº§n so sÃ¡nh ngáº¯n giá»¯a pre-trained vÃ  self-trained báº±ng cÃ¡ch cháº¡y demo self-trained trÃªn EWT vÃ  ghi nháº­n khÃ¡c biá»‡t vÃ o bÃ¡o cÃ¡o.
- ThÃªm test smoke CI (lÆ°u Ã½: cÃ³ thá»ƒ cáº§n táº£i máº¡ng Ä‘á»ƒ láº¥y model pre-trained).

Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ (A) thÃªm code visualization + táº¡o áº£nh máº«u (tÃ´i sáº½ commit chÃºng), vÃ  (B) bá»• sung pháº§n so sÃ¡nh sau khi cháº¡y demo self-trained â€” báº¡n muá»‘n tÃ´i lÃ m pháº§n nÃ o trÆ°á»›c? âœ…
