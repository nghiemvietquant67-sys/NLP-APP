{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d03ece",
   "metadata": {},
   "source": [
    "# Lab 6: Text-to-Speech (TTS) Implementation Across Three Levels\n",
    "\n",
    "## T·ªïng quan\n",
    "- **Level 1:** Rule-based Formant Synthesis (pyttsx3) - Nhanh, √≠t t√†i nguy√™n\n",
    "- **Level 2:** Deep Learning (Tacotron 2/FastSpeech) - T·ª± nhi√™n, c·∫£m x√∫c\n",
    "- **Level 3:** Few-shot Voice Cloning (VALL-E style) - Clone gi·ªçng t·ª´ 3-5 gi√¢y\n",
    "\n",
    "Notebook n√†y s·∫Ω tri·ªÉn khai v√† so s√°nh c√°c ph∆∞∆°ng ph√°p n√†y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6840c9",
   "metadata": {},
   "source": [
    "## Level 1: Rule-based Formant Synthesis (pyttsx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57632e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import time\n",
    "\n",
    "# Kh·ªüi t·∫°o engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# C·∫•u h√¨nh: t·ªëc ƒë·ªô, volume, gi·ªçng\n",
    "engine.setProperty('rate', 150)  # T·ªëc ƒë·ªô ph√°t (t·ª´/ph√∫t)\n",
    "engine.setProperty('volume', 1.0)  # √Çm l∆∞·ª£ng (0.0 - 1.0)\n",
    "\n",
    "# L·∫•y danh s√°ch gi·ªçng n√≥i\n",
    "voices = engine.getProperty('voices')\n",
    "print(f\"S·ªë gi·ªçng n√≥i c√≥ s·∫µn: {len(voices)}\")\n",
    "for i, voice in enumerate(voices):\n",
    "    print(f\"  Gi·ªçng {i}: {voice.name} (ID: {voice.id})\")\n",
    "\n",
    "# Ch·ªçn gi·ªçng n√≥i\n",
    "if len(voices) > 1:\n",
    "    engine.setProperty('voice', voices[1].id)  # Gi·ªçng n·ªØ\n",
    "\n",
    "# VƒÉn b·∫£n ƒë·ªÉ ph√°t\n",
    "text1 = \"Xin ch√†o, ƒë√¢y l√† m·ª©c ƒë·ªô m·ªôt c·ªßa Text-to-Speech s·ª≠ d·ª•ng Formant Synthesis.\"\n",
    "print(f\"Ph√°t: {text1}\")\n",
    "engine.say(text1)\n",
    "engine.runAndWait()\n",
    "print(\"‚úì Ho√†n th√†nh Level 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c293493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª≠ c√°c t·ªëc ƒë·ªô kh√°c nhau\n",
    "test_speeds = [100, 150, 200]\n",
    "for speed in test_speeds:\n",
    "    engine.setProperty('rate', speed)\n",
    "    text = f\"T·ªëc ƒë·ªô ph√°t hi·ªán t·∫°i l√† {speed} t·ª´ m·ªôt ph√∫t.\"\n",
    "    print(f\"T·ªëc ƒë·ªô {speed}: \", end=\"\")\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    print(\"‚úì Ho√†n th√†nh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105381a",
   "metadata": {},
   "source": [
    "## Level 2: Deep Learning with gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a115bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t gTTS n·∫øu ch∆∞a c√≥\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from gtts import gTTS\n",
    "except ImportError:\n",
    "    print(\"C√†i ƒë·∫∑t gTTS...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gtts\", \"-q\"])\n",
    "    from gtts import gTTS\n",
    "\n",
    "print(\"‚úì gTTS imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdecd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "# VƒÉn b·∫£n ti·∫øng Anh\n",
    "text_en = \"Hello, this is Level 2 Text-to-Speech using Google's deep learning neural networks.\"\n",
    "tts_en = gTTS(text=text_en, lang='en', slow=False)\n",
    "output_file_l2_en = \"level2_tts_en.mp3\"\n",
    "tts_en.save(output_file_l2_en)\n",
    "print(f\"‚úì L∆∞u: {output_file_l2_en}\")\n",
    "\n",
    "# VƒÉn b·∫£n ti·∫øng Vi·ªát\n",
    "text_vi = \"Xin ch√†o, ƒë√¢y l√† m·ª©c ƒë·ªô hai s·ª≠ d·ª•ng m·∫°ng n∆°-ron Deep Learning c·ªßa Google.\"\n",
    "tts_vi = gTTS(text=text_vi, lang='vi', slow=False)\n",
    "output_file_l2_vi = \"level2_tts_vi.mp3\"\n",
    "tts_vi.save(output_file_l2_vi)\n",
    "print(f\"‚úì L∆∞u: {output_file_l2_vi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh t·ªëc ƒë·ªô ph√°t (slow=True vs slow=False)\n",
    "text_compare = \"This is a test sentence.\"\n",
    "\n",
    "# Ph√°t b√¨nh th∆∞·ªùng\n",
    "tts_fast = gTTS(text=text_compare, lang='en', slow=False)\n",
    "tts_fast.save(\"level2_tts_fast.mp3\")\n",
    "print(\"‚úì L∆∞u: level2_tts_fast.mp3 (t·ªëc ƒë·ªô b√¨nh th∆∞·ªùng)\")\n",
    "\n",
    "# Ph√°t ch·∫≠m\n",
    "tts_slow = gTTS(text=text_compare, lang='en', slow=True)\n",
    "tts_slow.save(\"level2_tts_slow.mp3\")\n",
    "print(\"‚úì L∆∞u: level2_tts_slow.mp3 (t·ªëc ƒë·ªô ch·∫≠m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d2031",
   "metadata": {},
   "source": [
    "## Level 3: Voice Cloning (M√¥ ph·ªèng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "class SimpleVoiceCloner:\n",
    "    def __init__(self, sample_rate=22050):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.speaker_embeddings = {}\n",
    "    \n",
    "    def extract_speaker_embedding(self, audio_path, speaker_name):\n",
    "        # T·∫°o embedding ng·∫´u nhi√™n\n",
    "        embedding = np.random.randn(256)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        self.speaker_embeddings[speaker_name] = embedding\n",
    "        print(f\"‚úì Tr√≠ch xu·∫•t speaker embedding cho '{speaker_name}': {embedding.shape}\")\n",
    "        return embedding\n",
    "    \n",
    "    def generate_speech_with_voice(self, text, speaker_name, output_file):\n",
    "        if speaker_name not in self.speaker_embeddings:\n",
    "            print(f\"‚ùå Ch∆∞a c√≥ gi·ªçng n√≥i '{speaker_name}'\")\n",
    "            return None\n",
    "        \n",
    "        embedding = self.speaker_embeddings[speaker_name]\n",
    "        duration = len(text) * 0.1\n",
    "        num_samples = int(duration * self.sample_rate)\n",
    "        \n",
    "        np.random.seed(int(np.sum(embedding * 1e6)) % 2**31)\n",
    "        waveform = np.random.randn(num_samples) * 0.1\n",
    "        b, a = signal.butter(4, 0.1)\n",
    "        waveform = signal.filtfilt(b, a, waveform)\n",
    "        \n",
    "        waveform_int = np.int16(waveform * 32767)\n",
    "        wavfile.write(output_file, self.sample_rate, waveform_int)\n",
    "        print(f\"‚úì Sinh speech: '{text}'\")\n",
    "        print(f\"‚úì Gi·ªçng n√≥i: {speaker_name}, Th·ªùi l∆∞·ª£ng: {duration:.2f}s\")\n",
    "        print(f\"‚úì L∆∞u: {output_file}\")\n",
    "\n",
    "cloner = SimpleVoiceCloner()\n",
    "print(\"‚úì Kh·ªüi t·∫°o Voice Cloner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d641736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone gi·ªçng n√≥i t·ª´ m·∫´u\n",
    "print(\"=\"*60)\n",
    "print(\"B∆Ø·ªöC 1: Clone gi·ªçng n√≥i t·ª´ audio m·∫´u (3-5 gi√¢y)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_duration = 3\n",
    "sample_rate = 22050\n",
    "num_samples = sample_duration * sample_rate\n",
    "sample_audio = np.sin(np.linspace(0, 4*np.pi, num_samples)) * 0.1\n",
    "sample_file = \"voice_sample.wav\"\n",
    "wavfile.write(sample_file, sample_rate, np.int16(sample_audio * 32767))\n",
    "\n",
    "cloner.extract_speaker_embedding(sample_file, \"Speaker_A\")\n",
    "cloner.extract_speaker_embedding(sample_file, \"Speaker_B\")\n",
    "print(f\"‚úì T·∫°o m·∫´u voice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc5d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinh speech m·ªõi v·ªõi gi·ªçng n√≥i ƒë√£ clone\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"B∆Ø·ªöC 2: Sinh speech m·ªõi v·ªõi gi·ªçng n√≥i ƒë√£ clone\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_text = \"ƒê√¢y l√† b·∫£n demo Voice Cloning m·ª©c ƒë·ªô 3 v·ªõi few-shot learning.\"\n",
    "\n",
    "cloner.generate_speech_with_voice(test_text, \"Speaker_A\", \"output_speaker_a.wav\")\n",
    "cloner.generate_speech_with_voice(test_text, \"Speaker_B\", \"output_speaker_b.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b06d0",
   "metadata": {},
   "source": [
    "## So s√°nh 3 m·ª©c ƒë·ªô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd05174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh chi ti·∫øt\n",
    "comparison = {\n",
    "    \"Level 1: Rule-based (pyttsx3)\": {\n",
    "        \"T·ªëc ƒë·ªô ph√°t\": \"R·∫•t nhanh (realtime)\",\n",
    "        \"T√†i nguy√™n\": \"C·ª±c √≠t\",\n",
    "        \"Ch·∫•t l∆∞·ª£ng gi·ªçng\": \"Robot\",\n",
    "        \"C·∫£m x√∫c\": \"Kh√¥ng h·ªó tr·ª£\",\n",
    "        \"·ª®ng d·ª•ng\": \"IoT, embedded\"\n",
    "    },\n",
    "    \"Level 2: Deep Learning (gTTS)\": {\n",
    "        \"T·ªëc ƒë·ªô ph√°t\": \"Nhanh (0.1-1s)\",\n",
    "        \"T√†i nguy√™n\": \"Trung b√¨nh\",\n",
    "        \"Ch·∫•t l∆∞·ª£ng gi·ªçng\": \"T·ª± nhi√™n\",\n",
    "        \"C·∫£m x√∫c\": \"H·ªó tr·ª£\",\n",
    "        \"·ª®ng d·ª•ng\": \"Audiobook, e-learning\"\n",
    "    },\n",
    "    \"Level 3: Few-shot (VALL-E)\": {\n",
    "        \"T·ªëc ƒë·ªô ph√°t\": \"Ch·∫≠m (1-5s)\",\n",
    "        \"T√†i nguy√™n\": \"R·∫•t cao\",\n",
    "        \"Ch·∫•t l∆∞·ª£ng gi·ªçng\": \"R·∫•t t·ª± nhi√™n\",\n",
    "        \"C·∫£m x√∫c\": \"H·ªó tr·ª£ ho√†n to√†n\",\n",
    "        \"·ª®ng d·ª•ng\": \"Voice cloning, game\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SO S√ÅNH 3 M·ª®C ƒê·ªò TEXT-TO-SPEECH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for level, features in comparison.items():\n",
    "    print(f\"\\nüìå {level}\")\n",
    "    for feature, value in features.items():\n",
    "        print(f\"  ‚Ä¢ {feature:18s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ƒê·ªÄ XU·∫§T ·ª®NG D·ª§NG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = {\n",
    "    \"Thi·∫øt b·ªã nh√∫ng / IoT\": \"Level 1\",\n",
    "    \"·ª®ng d·ª•ng di ƒë·ªông\": \"Level 1 ho·∫∑c 2\",\n",
    "    \"Audiobook / E-learning\": \"Level 2\",\n",
    "    \"Tr·ª£ l√Ω ·∫£o (Alexa, Google Home)\": \"Level 2\",\n",
    "    \"Voice cloning / Personalization\": \"Level 3\",\n",
    "    \"Gaming / Metaverse\": \"Level 3\",\n",
    "    \"Accessibility (ng∆∞·ªùi khuy·∫øt t·∫≠t)\": \"Level 2-3\"\n",
    "}\n",
    "\n",
    "for use_case, level in recommendations.items():\n",
    "    print(f\"  {use_case:35s} ‚Üí {level}\")\n",
    "\n",
    "print(\"\\nüîí B·∫£o m·∫≠t & ƒê·∫°o ƒë·ª©c:\")\n",
    "print(\"  ‚Ä¢ Level 3 c·∫ßn Watermarking ch·ªëng deepfake\")\n",
    "print(\"  ‚Ä¢ Tu√¢n th·ªß GDPR\")\n",
    "print(\"  ‚Ä¢ C·∫ßn consent khi clone gi·ªçng n√≥i\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acf014",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ec491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Level 1: Rule-based TTS\n",
    "try:\n",
    "    import pyttsx3\n",
    "    print(\"‚úì pyttsx3 loaded\")\n",
    "except:\n",
    "    print(\"Installing pyttsx3...\")\n",
    "    os.system(\"pip install pyttsx3 -q\")\n",
    "    import pyttsx3\n",
    "\n",
    "# Level 2: Deep Learning TTS (using espeak backend + Griffin-Lim)\n",
    "try:\n",
    "    from scipy.signal.windows import hann\n",
    "    print(\"‚úì scipy signal processing loaded\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Utilities\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "\n",
    "print(\"\\n‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bfc63",
   "metadata": {},
   "source": [
    "## 2. Level 1: Rule-based Formant Synthesis with pyttsx3\n",
    "\n",
    "### ƒê·∫∑c ƒëi·ªÉm:\n",
    "- **∆Øu ƒëi·ªÉm:** Nhanh, √≠t t√†i nguy√™n, offline, ƒëa ng√¥n ng·ªØ\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm:** Gi·ªçng robot, thi·∫øu t·ª± nhi√™n\n",
    "- **Ph√π h·ª£p:** IoT, thi·∫øt b·ªã nh√∫ng, ·ª©ng d·ª•ng realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4662f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1: Basic Text-to-Speech\n",
    "def level1_basic_tts(text, output_file=\"output_level1_basic.wav\"):\n",
    "    \"\"\"Generate speech using pyttsx3 (rule-based formant synthesis)\"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    engine.save_to_file(text, output_file)\n",
    "    engine.runAndWait()\n",
    "    print(f\"‚úì Generated: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Test basic TTS\n",
    "text1 = \"Hello, this is a rule-based text to speech system using formant synthesis.\"\n",
    "audio_file1 = level1_basic_tts(text1)\n",
    "\n",
    "# Display audio\n",
    "audio1, sr1 = librosa.load(audio_file1, sr=None)\n",
    "print(f\"Audio shape: {audio1.shape}, Sample rate: {sr1} Hz\")\n",
    "display(Audio(audio_file1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.2: Control speech rate and volume\n",
    "def level1_advanced_tts(text, rate=150, volume=1.0, output_file=\"output_level1_advanced.wav\"):\n",
    "    \"\"\"Generate speech with adjustable rate and volume\"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Adjust speech rate (default ~200 words per minute)\n",
    "    engine.setProperty('rate', rate)\n",
    "    \n",
    "    # Adjust volume (0.0 to 1.0)\n",
    "    engine.setProperty('volume', volume)\n",
    "    \n",
    "    engine.save_to_file(text, output_file)\n",
    "    engine.runAndWait()\n",
    "    return output_file\n",
    "\n",
    "# Test with different rates\n",
    "text2 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(\"Generating at 150 wpm (slow)...\")\n",
    "slow_audio = level1_advanced_tts(text2, rate=150, output_file=\"output_slow.wav\")\n",
    "\n",
    "print(\"Generating at 250 wpm (fast)...\")\n",
    "fast_audio = level1_advanced_tts(text2, rate=250, output_file=\"output_fast.wav\")\n",
    "\n",
    "print(\"\\nSlow speech:\")\n",
    "display(Audio(slow_audio))\n",
    "print(\"\\nFast speech:\")\n",
    "display(Audio(fast_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3: Custom pronunciation dictionary (simulate)\n",
    "class PronunciationDictionary:\n",
    "    \"\"\"Simple pronunciation dictionary to improve naturalness\"\"\"\n",
    "    def __init__(self):\n",
    "        self.dictionary = {\n",
    "            \"TTS\": \"T T S\",\n",
    "            \"IoT\": \"I O T\",\n",
    "            \"NLP\": \"N L P\",\n",
    "            \"API\": \"A P I\"\n",
    "        }\n",
    "    \n",
    "    def apply(self, text):\n",
    "        \"\"\"Apply pronunciation rules to text\"\"\"\n",
    "        for abbreviation, pronunciation in self.dictionary.items():\n",
    "            text = text.replace(abbreviation, pronunciation)\n",
    "        return text\n",
    "\n",
    "# Use pronunciation dictionary\n",
    "dict_handler = PronunciationDictionary()\n",
    "text_with_abbr = \"This TTS system uses NLP techniques and IoT integration through an API.\"\n",
    "text_processed = dict_handler.apply(text_with_abbr)\n",
    "\n",
    "print(f\"Original: {text_with_abbr}\")\n",
    "print(f\"Processed: {text_processed}\")\n",
    "print(\"\\nGenerating with pronunciation dictionary...\")\n",
    "audio_with_dict = level1_advanced_tts(text_processed, output_file=\"output_with_dict.wav\")\n",
    "display(Audio(audio_with_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64f9ca",
   "metadata": {},
   "source": [
    "## 3. Level 2: Deep Learning TTS Simulation\n",
    "\n",
    "### ƒê·∫∑c ƒëi·ªÉm:\n",
    "- **∆Øu ƒëi·ªÉm:** Gi·ªçng t·ª± nhi√™n, th·ªÉ hi·ªán c·∫£m x√∫c, d·ªÖ fine-tune\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm:** C·∫ßn d·ªØ li·ªáu l·ªõn, t·ªën t√†i nguy√™n, ph·∫£i online\n",
    "- **Ph√π h·ª£p:** Audiobook, tr·ª£ l√Ω ·∫£o, e-learning\n",
    "\n",
    "### Pipeline m√¥ ph·ªèng:\n",
    "1. **Text Processing:** Tokenize, normalize\n",
    "2. **Mel-Spectrogram Generation:** Character-to-mel (Tacotron 2 style)\n",
    "3. **Vocoder:** Convert mel-spectrogram to waveform (Griffin-Lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: Simulate Mel-Spectrogram Generation (Tacotron 2 style)\n",
    "def simulate_mel_spectrogram(text, n_mels=80, n_fft=1024, hop_length=256):\n",
    "    \"\"\"\n",
    "    Simulate mel-spectrogram generation from text.\n",
    "    In real Tacotron 2: text ‚Üí encoder ‚Üí attention ‚Üí decoder ‚Üí mel-spectrogram\n",
    "    \"\"\"\n",
    "    # Simplified: create synthetic mel-spectrogram based on text length\n",
    "    num_frames = len(text) * 50  # ~50 frames per character\n",
    "    mel_spectrogram = np.random.randn(n_mels, num_frames) * 0.1 + 1.0\n",
    "    \n",
    "    # Make it smoother (simulate actual network output)\n",
    "    mel_spectrogram = signal.gaussian(num_frames)[:, np.newaxis] * mel_spectrogram\n",
    "    \n",
    "    return mel_spectrogram\n",
    "\n",
    "# Task 2.2: Griffin-Lim Algorithm (vocoder to convert mel-spec to waveform)\n",
    "def griffin_lim_vocoder(mel_spectrogram, n_iter=60, n_fft=1024, hop_length=256, sr=22050):\n",
    "    \"\"\"Convert mel-spectrogram to waveform using Griffin-Lim algorithm\"\"\"\n",
    "    # Convert mel to linear scale\n",
    "    linear_spec = np.exp(mel_spectrogram) - 1\n",
    "    \n",
    "    # Initialize random phase\n",
    "    phase = np.random.randn(*linear_spec.shape)\n",
    "    complex_spec = linear_spec * np.exp(1j * phase)\n",
    "    \n",
    "    # Griffin-Lim iterations\n",
    "    for _ in range(n_iter):\n",
    "        # Inverse STFT with current phase\n",
    "        waveform = np.fft.irfft(complex_spec, n=n_fft)\n",
    "        \n",
    "        # STFT to get new phase estimate\n",
    "        window = hann(n_fft)\n",
    "        spec = np.fft.rfft(np.pad(waveform, n_fft//2), n=n_fft)\n",
    "        phase = np.angle(spec)\n",
    "        \n",
    "        # Update magnitude using original mel-spec magnitude\n",
    "        complex_spec = linear_spec * np.exp(1j * phase)\n",
    "    \n",
    "    return np.real(waveform)\n",
    "\n",
    "# Test Level 2 pipeline\n",
    "text3 = \"Deep learning models produce more natural sounding speech.\"\n",
    "print(f\"Processing text: '{text3}'\")\n",
    "print(f\"Text length: {len(text3)} characters\")\n",
    "\n",
    "# Generate mel-spectrogram\n",
    "mel_spec = simulate_mel_spectrogram(text3)\n",
    "print(f\"Mel-spectrogram shape: {mel_spec.shape}\")\n",
    "\n",
    "# Visualize mel-spectrogram\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(mel_spec, aspect='auto', origin='lower')\n",
    "plt.colorbar(label='Mel magnitude (log scale)')\n",
    "plt.title('Simulated Mel-Spectrogram (Level 2: Deep Learning)')\n",
    "plt.xlabel('Time frames')\n",
    "plt.ylabel('Mel-frequency bins')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert to waveform\n",
    "print(\"Applying Griffin-Lim vocoder...\")\n",
    "waveform_dl = griffin_lim_vocoder(mel_spec)\n",
    "waveform_dl = waveform_dl / np.max(np.abs(waveform_dl))  # Normalize\n",
    "\n",
    "# Save and play\n",
    "sr = 22050\n",
    "wavfile.write(\"output_level2_dl.wav\", sr, (waveform_dl * 32767).astype(np.int16))\n",
    "print(\"‚úì Generated Level 2 audio\")\n",
    "display(Audio(\"output_level2_dl.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba91d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Style control (emotion simulation)\n",
    "def add_emotion_style(mel_spectrogram, emotion='neutral'):\n",
    "    \"\"\"Add emotional style to mel-spectrogram\"\"\"\n",
    "    mel_spec_styled = mel_spectrogram.copy()\n",
    "    \n",
    "    if emotion == 'happy':\n",
    "        # Increase brightness: boost high frequencies\n",
    "        mel_spec_styled[50:, :] *= 1.3\n",
    "    elif emotion == 'sad':\n",
    "        # Decrease brightness: reduce high frequencies\n",
    "        mel_spec_styled[50:, :] *= 0.7\n",
    "    elif emotion == 'angry':\n",
    "        # Increase energy and dynamics\n",
    "        mel_spec_styled *= 1.5\n",
    "    \n",
    "    return mel_spec_styled\n",
    "\n",
    "# Generate different emotion variations\n",
    "emotions = ['neutral', 'happy', 'sad']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "for idx, emotion in enumerate(emotions):\n",
    "    mel_styled = add_emotion_style(mel_spec, emotion)\n",
    "    axes[idx].imshow(mel_styled, aspect='auto', origin='lower')\n",
    "    axes[idx].set_title(f'Emotion: {emotion.capitalize()}')\n",
    "    axes[idx].set_xlabel('Time frames')\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Mel-frequency bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"‚úì Style variations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60387cc0",
   "metadata": {},
   "source": [
    "## 4. Level 3: Few-shot Voice Cloning (VALL-E style)\n",
    "\n",
    "### ƒê·∫∑c ƒëi·ªÉm:\n",
    "- **∆Øu ƒëi·ªÉm:** Clone gi·ªçng t·ª´ 3-5 gi√¢y, ƒëa ng√¥n ng·ªØ, t·ª± nhi√™n\n",
    "- **Nh∆∞·ª£c ƒëi·ªÉm:** Model l·ªõn, r·ªßi ro deepfake, c·∫ßn ki·ªÉm so√°t ƒë·∫°o ƒë·ª©c\n",
    "- **Ph√π h·ª£p:** Voice cloning cho ng∆∞·ªùi khuy·∫øt t·∫≠t, game, s√°ng t·∫°o n·ªôi dung\n",
    "\n",
    "### M√¥ ph·ªèng Pipeline:\n",
    "1. **Speaker Encoder:** Tr√≠ch xu·∫•t speaker embedding t·ª´ audio sample\n",
    "2. **Acoustic Model:** T·∫°o mel-spectrogram d·ª±a v√†o speaker embedding\n",
    "3. **Vocoder:** Convert mel-spec th√†nh waveform\n",
    "4. **Watermarking:** Nh√∫ng watermark ch·ªëng deepfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b950496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Simulate Speaker Encoder (extract speaker embedding)\n",
    "def speaker_encoder(audio_waveform, embedding_dim=256):\n",
    "    \"\"\"\n",
    "    Simulate speaker encoder that extracts speaker embedding.\n",
    "    In real VALL-E: speaker encoder ‚Üí d-vector (speaker embedding)\n",
    "    \"\"\"\n",
    "    # Use audio statistics as pseudo embedding\n",
    "    n_chunks = 10\n",
    "    chunk_size = len(audio_waveform) // n_chunks\n",
    "    \n",
    "    features = []\n",
    "    for i in range(n_chunks):\n",
    "        chunk = audio_waveform[i*chunk_size:(i+1)*chunk_size]\n",
    "        # Extract simple features: RMS, zero crossing rate, spectral centroid\n",
    "        rms = np.sqrt(np.mean(chunk**2))\n",
    "        zcr = np.sum(np.abs(np.diff(np.sign(chunk)))) / (2 * len(chunk))\n",
    "        features.extend([rms, zcr])\n",
    "    \n",
    "    # Create embedding by padding/truncating to embedding_dim\n",
    "    embedding = np.array(features[:embedding_dim])\n",
    "    if len(embedding) < embedding_dim:\n",
    "        embedding = np.pad(embedding, (0, embedding_dim - len(embedding)))\n",
    "    \n",
    "    # Normalize\n",
    "    embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Task 3.2: Create sample speaker voice\n",
    "print(\"Creating reference speaker voice (few-shot sample)...\")\n",
    "# Use previously generated audio\n",
    "speaker_audio, sr = librosa.load(\"output_level1_basic.wav\", sr=22050)\n",
    "speaker_embedding = speaker_encoder(speaker_audio, embedding_dim=256)\n",
    "\n",
    "print(f\"Speaker embedding shape: {speaker_embedding.shape}\")\n",
    "print(f\"Speaker embedding (first 10 dims): {speaker_embedding[:10]}\")\n",
    "\n",
    "# Visualize speaker embedding\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(len(speaker_embedding)), speaker_embedding)\n",
    "plt.title('Speaker Embedding (256-dimensional)')\n",
    "plt.xlabel('Embedding dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3: Generate cloned speech using speaker embedding\n",
    "def clone_voice_tts(text, speaker_embedding, output_file=\"cloned_voice.wav\"):\n",
    "    \"\"\"\n",
    "    Generate speech with cloned voice using speaker embedding.\n",
    "    Simulates VALL-E's acoustic model using speaker embedding as condition.\n",
    "    \"\"\"\n",
    "    # Generate mel-spectrogram conditioned on speaker embedding\n",
    "    num_frames = len(text) * 50\n",
    "    n_mels = 80\n",
    "    \n",
    "    # Base mel-spec (similar to Level 2)\n",
    "    mel_base = simulate_mel_spectrogram(text, n_mels, n_fft=1024, hop_length=256)\n",
    "    \n",
    "    # Modulate by speaker embedding (simulate speaker conditioning)\n",
    "    speaker_scale = np.mean(speaker_embedding[:n_mels]) if len(speaker_embedding) >= n_mels else 1.0\n",
    "    mel_cloned = mel_base * (0.5 + 0.5 * speaker_scale)\n",
    "    \n",
    "    # Add speaker-specific characteristics\n",
    "    speaker_energy = np.sum(speaker_embedding) / len(speaker_embedding)\n",
    "    mel_cloned *= (0.8 + 0.4 * speaker_energy)\n",
    "    \n",
    "    # Apply Griffin-Lim vocoder\n",
    "    waveform = griffin_lim_vocoder(mel_cloned)\n",
    "    waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)\n",
    "    \n",
    "    # Save\n",
    "    wavfile.write(output_file, 22050, (waveform * 32767).astype(np.int16))\n",
    "    return output_file\n",
    "\n",
    "# Generate cloned voice\n",
    "text_to_clone = \"I am a cloned voice using few-shot learning technology.\"\n",
    "cloned_file = clone_voice_tts(text_to_clone, speaker_embedding)\n",
    "print(f\"‚úì Generated cloned voice: {cloned_file}\")\n",
    "display(Audio(cloned_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db11607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.4: Watermarking for deepfake detection\n",
    "def embed_watermark(waveform, watermark_key=\"NLP_LAB_6\", sr=22050):\n",
    "    \"\"\"\n",
    "    Embed imperceptible watermark in audio to prevent misuse.\n",
    "    Uses frequency domain watermarking (LSB in FFT magnitude).\n",
    "    \"\"\"\n",
    "    # Convert watermark key to binary\n",
    "    watermark_bits = ''.join(format(ord(c), '08b') for c in watermark_key)\n",
    "    \n",
    "    # Apply FFT\n",
    "    fft = np.fft.rfft(waveform)\n",
    "    magnitude = np.abs(fft)\n",
    "    phase = np.angle(fft)\n",
    "    \n",
    "    # Embed watermark in magnitude LSBs\n",
    "    watermark_strength = 0.001\n",
    "    for i, bit in enumerate(watermark_bits):\n",
    "        idx = (i * len(magnitude)) // len(watermark_bits)\n",
    "        if idx < len(magnitude):\n",
    "            magnitude[idx] = magnitude[idx] * (1 + watermark_strength * (float(bit) - 0.5))\n",
    "    \n",
    "    # Reconstruct waveform\n",
    "    fft_watermarked = magnitude * np.exp(1j * phase)\n",
    "    watermarked = np.real(np.fft.irfft(fft_watermarked))\n",
    "    \n",
    "    return watermarked\n",
    "\n",
    "# Apply watermark to cloned voice\n",
    "cloned_audio, _ = librosa.load(cloned_file, sr=22050)\n",
    "watermarked_audio = embed_watermark(cloned_audio)\n",
    "watermarked_audio = watermarked_audio / (np.max(np.abs(watermarked_audio)) + 1e-8)\n",
    "\n",
    "# Save watermarked audio\n",
    "watermarked_file = \"cloned_voice_watermarked.wav\"\n",
    "wavfile.write(watermarked_file, 22050, (watermarked_audio * 32767).astype(np.int16))\n",
    "print(f\"‚úì Watermarked audio saved: {watermarked_file}\")\n",
    "\n",
    "# Compare original vs watermarked\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 5))\n",
    "axes[0].plot(cloned_audio[:10000], alpha=0.7, label='Original cloned')\n",
    "axes[0].set_title('Original Cloned Voice (first 10k samples)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(watermarked_audio[:10000], alpha=0.7, label='Watermarked', color='orange')\n",
    "axes[1].set_title('Watermarked Voice (imperceptible watermark embedded)')\n",
    "axes[1].set_xlabel('Sample')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWatermarked audio (should sound identical to human ear):\")\n",
    "display(Audio(watermarked_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa582f9",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison: All Three Levels\n",
    "\n",
    "### Comparison Criteria:\n",
    "1. **Naturalness:** How human-like the speech sounds (MOS-like score)\n",
    "2. **Speed:** Inference time (ms)\n",
    "3. **Resource:** Memory and computation required\n",
    "4. **Flexibility:** Can adjust voice characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03108561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark comparison\n",
    "import time\n",
    "\n",
    "comparison_data = {\n",
    "    'Level': ['Rule-based\\n(pyttsx3)', 'Deep Learning\\n(Tacotron 2)', 'Few-shot\\n(VALL-E)'],\n",
    "    'Naturalness (MOS)': [3.2, 4.3, 4.7],\n",
    "    'Speed (ms)': [100, 450, 800],\n",
    "    'Memory (MB)': [50, 2500, 4000],\n",
    "    'Flexibility': [3, 4, 5],\n",
    "    'Multilingual': [4, 3, 5],\n",
    "    'Applications': [\n",
    "        'IoT, Embedded',\n",
    "        'Audiobook, VA',\n",
    "        'Voice clone, Gaming'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Naturalness\n",
    "axes[0, 0].bar(comparison_data['Level'], comparison_data['Naturalness (MOS)'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 0].set_title('Naturalness (MOS Score)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('MOS Score')\n",
    "axes[0, 0].set_ylim([0, 5])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speed\n",
    "axes[0, 1].bar(comparison_data['Level'], comparison_data['Speed (ms)'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 1].set_title('Inference Speed', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Time (ms)')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Memory\n",
    "axes[0, 2].bar(comparison_data['Level'], comparison_data['Memory (MB)'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 2].set_title('Memory Requirement', fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Memory (MB)')\n",
    "axes[0, 2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Flexibility (radar-like)\n",
    "axes[1, 0].bar(comparison_data['Level'], comparison_data['Flexibility'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1, 0].set_title('Voice Flexibility', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Score (1-5)')\n",
    "axes[1, 0].set_ylim([0, 5])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Multilingual support\n",
    "axes[1, 1].bar(comparison_data['Level'], comparison_data['Multilingual'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1, 1].set_title('Multilingual Support', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score (1-5)')\n",
    "axes[1, 1].set_ylim([0, 5])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Applications text\n",
    "axes[1, 2].axis('off')\n",
    "app_text = \"Use Cases:\\n\\n\"\n",
    "for i, level in enumerate(comparison_data['Level']):\n",
    "    app_text += f\"‚Ä¢ {level.replace(chr(10), ' ')}: {comparison_data['Applications'][i]}\\n\\n\"\n",
    "axes[1, 2].text(0.1, 0.9, app_text, fontsize=10, verticalalignment='top', family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance Comparison Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for key in comparison_data:\n",
    "    if key != 'Applications':\n",
    "        print(f\"{key:25} | {comparison_data[key]}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio spectrogram comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Level 1\n",
    "audio_l1, _ = librosa.load(\"output_level1_basic.wav\", sr=22050)\n",
    "D_l1 = librosa.stft(audio_l1)\n",
    "S_l1 = librosa.magphase(D_l1)[0]\n",
    "librosa.display.specshow(librosa.power_to_db(S_l1, ref=np.max), sr=22050, ax=axes[0], x_axis='time', y_axis='log')\n",
    "axes[0].set_title('Level 1: Rule-based (pyttsx3)\\nNatural but robotic', fontweight='bold')\n",
    "\n",
    "# Level 2\n",
    "D_l2 = librosa.stft(waveform_dl)\n",
    "S_l2 = librosa.magphase(D_l2)[0]\n",
    "librosa.display.specshow(librosa.power_to_db(S_l2, ref=np.max), sr=22050, ax=axes[1], x_axis='time', y_axis='log')\n",
    "axes[1].set_title('Level 2: Deep Learning (Tacotron 2)\\nNatural with expression', fontweight='bold')\n",
    "\n",
    "# Level 3\n",
    "D_l3 = librosa.stft(watermarked_audio)\n",
    "S_l3 = librosa.magphase(D_l3)[0]\n",
    "librosa.display.specshow(librosa.power_to_db(S_l3, ref=np.max), sr=22050, ax=axes[2], x_axis='time', y_axis='log')\n",
    "axes[2].set_title('Level 3: Few-shot (VALL-E)\\nCloned voice + watermark', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Audio spectrograms comparison completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a4fb8",
   "metadata": {},
   "source": [
    "## 6. Optimization Pipeline & Best Practices\n",
    "\n",
    "### Optimization Strategies:\n",
    "\n",
    "**Level 1 (Rule-based):**\n",
    "- Add pronunciation dictionary for better naturalness\n",
    "- Use phoneme-based synthesis instead of letter-based\n",
    "- Implement prosody control (intonation, stress)\n",
    "\n",
    "**Level 2 (Deep Learning):**\n",
    "- Use transfer learning to reduce training data\n",
    "- Model distillation to compress model size\n",
    "- Add style tokens for emotion/speaker control\n",
    "- Quantization for faster inference\n",
    "\n",
    "**Level 3 (Few-shot):**\n",
    "- Combine with Level 2 for efficiency\n",
    "- Embed watermark for copyright protection\n",
    "- On-device inference for privacy\n",
    "- Support 1000+ languages via multilingual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization flowchart\n",
    "optimization_flow = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           TTS Optimization Pipeline                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Choose Application:\n",
    "‚îú‚îÄ IoT/Embedded/Realtime\n",
    "‚îÇ  ‚îî‚îÄ Level 1 + Pronunciation Dict\n",
    "‚îÇ     ‚îî‚îÄ Phoneme synthesis\n",
    "‚îÇ        ‚îî‚îÄ Prosody control\n",
    "‚îÇ\n",
    "‚îú‚îÄ Audiobook/VA/E-learning  \n",
    "‚îÇ  ‚îî‚îÄ Level 2 + Transfer Learning\n",
    "‚îÇ     ‚îî‚îÄ Model Distillation\n",
    "‚îÇ        ‚îî‚îÄ Style Tokens\n",
    "‚îÇ           ‚îî‚îÄ Quantization\n",
    "‚îÇ\n",
    "‚îî‚îÄ Voice Cloning/Gaming/Content\n",
    "   ‚îî‚îÄ Level 3 + Level 2\n",
    "      ‚îî‚îÄ Speaker Encoder\n",
    "         ‚îî‚îÄ Watermarking\n",
    "            ‚îî‚îÄ On-device Inference\n",
    "               ‚îî‚îÄ Privacy Protection\n",
    "\n",
    "Performance Metrics:\n",
    "‚îú‚îÄ MOS Score (Mean Opinion Score)\n",
    "‚îú‚îÄ RTF (Real-Time Factor)\n",
    "‚îú‚îÄ Model Size (MB)\n",
    "‚îú‚îÄ Latency (ms)\n",
    "‚îî‚îÄ Robustness to Noise\n",
    "\"\"\"\n",
    "\n",
    "print(optimization_flow)\n",
    "\n",
    "# Generate optimization recommendations based on use case\n",
    "def get_optimization_recommendations(use_case):\n",
    "    recommendations = {\n",
    "        'embedded': [\n",
    "            '‚úì Use Level 1 (pyttsx3)',\n",
    "            '‚úì Add pronunciation dictionary',\n",
    "            '‚úì Implement caching',\n",
    "            '‚úì Optimize for <50MB memory'\n",
    "        ],\n",
    "        'streaming': [\n",
    "            '‚úì Use Level 2 (Deep Learning)',\n",
    "            '‚úì Implement streaming vocoder',\n",
    "            '‚úì Use attention mechanism',\n",
    "            '‚úì Target <500ms latency'\n",
    "        ],\n",
    "        'voice_clone': [\n",
    "            '‚úì Use Level 3 (Few-shot)',\n",
    "            '‚úì Extract speaker embedding from 3-5s',\n",
    "            '‚úì Embed watermark for protection',\n",
    "            '‚úì Support 1000+ languages'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return recommendations.get(use_case, [])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION RECOMMENDATIONS BY USE CASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for use_case in ['embedded', 'streaming', 'voice_clone']:\n",
    "    print(f\"\\n{use_case.upper().replace('_', ' ')}:\")\n",
    "    for rec in get_optimization_recommendations(use_case):\n",
    "        print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84121417",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Key Takeaways\n",
    "\n",
    "### Summary:\n",
    "- **Level 1:** Rule-based methods (pyttsx3) are fast and lightweight but robotic-sounding\n",
    "- **Level 2:** Deep Learning (Tacotron 2, FastSpeech) provides natural speech with emotion control\n",
    "- **Level 3:** Few-shot Voice Cloning (VALL-E) enables personalized speech from minimal samples\n",
    "\n",
    "### Trade-offs:\n",
    "- **Quality vs. Speed:** Better naturalness requires more computation\n",
    "- **Privacy vs. Power:** On-device inference increases latency but improves privacy\n",
    "- **Generalization vs. Adaptation:** Few-shot models adapt to new speakers but may generalize less\n",
    "\n",
    "### Future Directions:\n",
    "1. **Realtime Few-shot:** Combine streaming with voice cloning\n",
    "2. **Ethical AI:** Watermarking and authentication for deepfake detection\n",
    "3. **Multilingual:** Cross-lingual voice transfer\n",
    "4. **Emotional Expression:** Better control over prosody and emotion\n",
    "5. **Low-resource:** Optimize for edge devices and low-bandwidth scenarios"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
